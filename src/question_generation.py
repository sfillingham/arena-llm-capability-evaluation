# Functions used to generate MCQs based on previously developed threat model
from typing import Callable, Literal, TypeAlias, Type
import time
import warnings
from tabulate import tabulate # type: ignore
import instructor # type: ignore
from concurrent.futures import ThreadPoolExecutor


Message: TypeAlias = dict[Literal["role", "content"], str]
Messages: TypeAlias = list[Message]


def retry_with_exponential_backoff(
    func,
    max_retries: int = 20,
    initial_sleep_time: float = 1.0,
    backoff_factor: float = 1.5,
) -> Callable:
    """
    Retry a function with exponential backoff.

    This decorator retries the wrapped function in case of rate limit errors, using an exponential backoff strategy to
    increase the wait time between retries.

    Args:
        func (callable): The function to be retried.
        max_retries (int): Maximum number of retry attempts. Defaults to 20.
        initial_sleep_time (float): Initial sleep time in seconds. Defaults to 1.
        backoff_factor (float): Factor by which the sleep time increases after each retry. Defaults to 1.5.

    Returns:
        callable: A wrapped version of the input function with retry logic.

    Raises:
        Exception: If the maximum number of retries is exceeded.
        Any other exception raised by the function that is not a rate limit error.

    Note:
        This function specifically handles rate limit and 529 errors. All other exceptions
        are re-raised immediately.
    """

    def wrapper(*args, **kwargs):
        sleep_time = initial_sleep_time

        for _ in range(max_retries):
          try:
            return func(*args, **kwargs)
          except Exception as e:
            if "rate limit" in str(e).lower().replace("_", " "):
              sleep_time *= backoff_factor
              time.sleep(sleep_time)
            elif "529" in str(e) or "overloaded" in str(e).lower():
              sleep_time *= backoff_factor
              time.sleep(sleep_time)
            else:
              raise e
        raise Exception("Exceeded the max number of retries")

    return wrapper


def generate_structured_response(
    model: str,
    messages: Messages, # type: ignore
    response_format: Type,
    temperature: float = 1,
    max_tokens: int = 2000,
    verbose: bool = False,
    stop_sequences: list[str] = [],
) -> dict:
    """
    Generate a response using the OpenAI or Anthropic APIs. The response is structured using the `response_format`
    parameter.

    Args:
        model (str): The name of the model to use (e.g., "gpt-4o-mini").
        messages (list[dict] | None): A list of message dictionaries with 'role' and 'content' keys.
        temperature (float): Controls randomness in output. Higher values make output more random. Default is 1.
        max_tokens (int): The maximum number of tokens to generate. Default is 1000.
        verbose (bool): If True, prints the input messages before making the API call. Default is False.
        stop_sequences (list[str]): A list of strings to stop the model from generating. Default is an empty list.

    Returns:
        dict: The model's response, as a dict with the same structure as the `response_format` class we pass in.
    """
    if model not in ["gpt-4o-mini", "claude-3-5-sonnet-20240620"]:
        warnings.warn(f"Warning: using unexpected model {model!r}")

    if verbose:
        print(tabulate([m.values() for m in messages], ["role", "content"], "simple_grid", maxcolwidths=[50, 70]))

    try:
        if "gpt" in model:
            response = openai_client.beta.chat.completions.parse(
                model=model,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                stop=stop_sequences,
                response_format=response_format,
            )
            return json.loads(response.choices[0].message.content)
        elif "claude" in model:
            # Extract system message if present
            has_system = messages[0]["role"] == "system"
            kwargs = {"system": messages[0]["content"]} if has_system else {}
            msgs = messages[1:] if has_system else messages

            response = instructor.from_anthropic(client=anthropic_client).messages.create(
                model=model,
                messages=msgs,
                temperature=temperature,
                max_tokens=max_tokens,
                stop_sequences=stop_sequences,
                response_model=response_format,
                **kwargs,
            )
            return response.model_dump()
        else:
            raise ValueError(f"Unknown model {model!r}")

    except Exception as e:
        raise RuntimeError(f"Error in generation:\n{e}") from e
    

def generate_structured_responses_with_threadpool(
    model: str,
    messages_list: list[Messages],
    response_format: Type,
    temperature: float = 1.0,
    max_tokens: int = 2000,
    verbose: bool = False,
    stop_sequences: list[str] = [],
    max_workers: int | None = 6,
) -> list[dict]:
    """
    Generate multiple responses using the OpenAI or Anthropic APIs, using `ThreadPoolExecutor` to execute the API calls
    concurrently. The response is structured using the `response_format` parameter.

    All arguments are the same as `generate_structured_response`, except:
        - `messages_list` is now a list of `Messages` objects, instead of a single `Messages` object.
        - `max_workers` is now a keyword argument, default 6. If it is None, then we don't use concurrency.

    Returns:
        list[dict]: The model's responses, as dicts with the same structure as the `response_format` class we pass in.
    """

    def generate_structured_response_wrapper(messages):
      return generate_structured_response(
            model=model,
            messages=messages,
            response_format=response_format,
            temperature=temperature,
            max_tokens=max_tokens,
            verbose=verbose,
            stop_sequences=stop_sequences,
        )

    if max_workers is None:
      results = map(generate_structured_response_wrapper, messages_list)

    else:
      with ThreadPoolExecutor(max_workers=max_workers) as executor:
        results = executor.map(generate_structured_response_wrapper, messages_list)

    return list(results)