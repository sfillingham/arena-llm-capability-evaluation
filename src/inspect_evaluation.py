# Functions used during Inspect evaluation
from inspect_ai.dataset import Sample, json_dataset # type: ignore
from inspect_ai.model import ChatMessageSystem, ChatMessageUser, get_model # type: ignore
from inspect_ai.scorer import answer # type: ignore
from inspect_ai.solver import Generate, Solver, TaskState, chain, solver, generate, Choices # type: ignore

from inspect_ai import Task, eval, task # type: ignore
import re
import random
from typing import Literal
from functools import partial


def record_to_sample(record: dict) -> Sample:
    """
    Converts a item ("record") from the dataset into a Sample object, mapping the fields of the record to the fields of
    the Sample object.

    Args:
        record : A dictionary from the json dataset containing our evaluation questions

    Returns:
        Sample : A Sample object containing the information in the record
    """
    input = [ChatMessageUser(content=record["question"])]
    has_system_prompt = record.get("system", "") != ""
    if has_system_prompt:
        input.insert(0, ChatMessageSystem(content=record["system"]))

    return Sample(
        input=input,
        target=record["answer_matching_behavior"],
        choices=list(record["answers"].values()),
        metadata={
            "labels": list(record["answers"].keys()),
            "behavior_category": record["question_category"],
            "system_prompt": has_system_prompt,
            "evasive_answer": record['evasive_answer'],
            "neutral_answer": record['neutral_answer'],
            "question_topic": record['question_topic'],
        },
    )


def record_to_sample_full(record: dict, system_prompt_behavior: Literal["system", "context"] | None = None) -> Sample:
    """
    Converts a item ("record") from the dataset into a Sample object, mapping the fields of the record to the fields
    of the Sample object.

    The ability to shuffle the responses has been commented out, needs to be fixed

    Args:
        record : A dictionary from the json dataset containing our evaluation questions

    Returns:
        Sample : A Sample object containing the information in the record
    """
    input = [ChatMessageUser(content=record["question"])]

    # Here is the changed code from `record_to_sample_shuffle`: we change the system prompt behaviour
    has_system_prompt = record.get("system", "") != "" and system_prompt_behavior is not None
    if has_system_prompt:
        if system_prompt_behavior == "system":
            input.insert(0, ChatMessageSystem(content=record["system"]))
        elif system_prompt_behavior == "context":
            input.insert(0, ChatMessageUser(content=f"Context:\n{record['system']}\n\n{record['question']}"))

    choices = list(record["answers"].values())
    target = record['answer_matching_behavior']
    # if random.choice([True, False]):
    #     choices = list(reversed(choices))
    #     target = record["answer_not_matching_behavior"]
    # else:
    #     target = record["answer_matching_behavior"]

    return Sample(
        input=input,
        target=target,
        choices=choices,
        metadata={
            "labels": list(record["answers"].keys()),
            "behavior_category": record["question_category"],
            "system_prompt": has_system_prompt,
            "evasive_answer": record['evasive_answer'],
            "neutral_answer": record['neutral_answer'],
            "question_topic": record['question_topic'],
        },
    )


@solver
def prompt_template(template: str) -> Solver:
    """
    Returns a solve function which modifies the user prompt with the given template.

    Args:
        template : The template string to use to modify the user prompt. Must include {prompt} to be replaced with the original user prompt.

    Returns:
        solve : A solve function which modifies the user prompt with the given template
    """
    # Check {prompt} is in the template, but no other fields
    assert set(re.findall(r"\{.*?\}", template)) == {r"{prompt}"}, r"Template must include {prompt} field and no others"

    async def solve(state: TaskState, generate: Generate) -> TaskState:
        state.user_prompt.text = template.format(prompt=state.user_prompt.text)

        return state

    return solve


def letters_and_answer_options(choices: Choices) -> tuple[str, str]:
    """
    Helper function, returns `choices` formatted as MCQ options, as well as the string of labels for each option.

    Example:

        ["choice 1", "choice 2", "choice 3"] -> (
            "A) choice 1\nB) choice 2\nC) choice 3",
            "A, B, C"
        )
    """
    letters = [chr(65 + i) for i in range(len(choices))]

    return (
        ", ".join(letters),
        "\n".join([f"{letter}) {choice.value}" for letter, choice in zip(letters, choices)]),
    )


@solver
def multiple_choice_format(template: str) -> Solver:
    """
    Returns a solve function which modifies the initial prompt to be in the format of an MCQ.

    Args:
        template: The template string to use to modify the user prompt. Must include {question} and {choices} to be replaced with the original user prompt and the answer choices, respectively.

    Returns:
        solve: A solve function which modifies the user prompt with the given template
    """
    tags = set(re.findall(r"\{.*?\}", template))
    assert r"{question}" in tags, "Template must include {question} field"
    assert r"{choices}" in tags, "Template must include {choices} field"
    assert tags - {r"{question}", r"{choices}", r"{letters}"} == set(), "Unexpected field found in template"

    async def solve(state: TaskState, generate: Generate) -> TaskState:
        assert state.choices, "If using MCQ then state must have `choices` field"

        letters, choices = letters_and_answer_options(state.choices)
        state.user_prompt.text = template.format(question=state.user_prompt.text, choices=choices, letters=letters)

        return state

    return solve


@solver
def make_choice(prompt: str) -> Solver:
    """
    Returns a solve function which adds a user message at the end of the state.messages list with the given prompt.

    Args:
        prompt : The prompt to add to the user messages (this will be an instruction to make a final choice).

    Returns:
        solve : A solve function which adds a user message with the given prompt to the end of the state.messages list
    """

    async def solve(state: TaskState, generate: Generate) -> TaskState:
        letters, _ = letters_and_answer_options(state.choices)
        state.messages.append(ChatMessageUser(content=prompt.format(letters=letters)))

        return state

    return solve


@solver
def self_critique_format(
    model_id: str,
    critique_template: str,
    critique_completion_template: str,
) -> Solver:
    """
    Generates a self-critique of the model's answer, as well as a new response based on the critique.

    Args:
        - model: The model we use to generate the self-critique
        - critique_template: Template asking model to produce a critique of the answer
        - critique_completion_template: Template asking model to generate a new improved answer based on the critique
    """
    model = get_model(model_id)

    async def solve(state: TaskState, generate: Generate) -> TaskState:
        # (1) Generate a critique of the model's answer
        letters, choices = letters_and_answer_options(state.choices)
        kwargs = dict(
            question=state.input_text,
            choices=choices,
            completion=state.output.completion,
            letters=letters,
        )
        critique = await model.generate(critique_template.format(**kwargs))

        # (2) Insert this critique into a new user message, at the end of current chat history
        state.messages.append(
            ChatMessageUser(content=critique_completion_template.format(**kwargs, critique=critique.completion))
        )

        return state

    return solve

@task
def benchmark_eval(json_dataset_path: str, 
                   n: int | None,
                   mcq_benchmark_template: str,
                   mcq_make_choice_template: str,
                   ) -> Task:
    """
    A task which carries out your benchmark evaluation.

    Args:
        json_dataset_path : The path where your dataset is saved
        n : the number of samples to use for the evaluation (if None then we use all of them)
        mcq_benchmark_template : This str should specify the instruction, question, and choices. See inspect_evaluation.ipynb notebook for an example
        make_choice_template : This str should specify how the model should make an answer choice and what format it should respond with.

    Returns:
        Task : A task which carries out your benchmark evaluation
    """
    return Task(
        dataset=json_dataset(
            json_file=json_dataset_path,
            sample_fields=partial(record_to_sample_full, system_prompt_behavior="context"),
            limit=n,
        ),
        solver=chain(
            multiple_choice_format(template=mcq_benchmark_template),
            generate(),
            make_choice(prompt=mcq_make_choice_template),
            generate(),
        ),
        scorer=answer("letter"),
    )


@task
def alignment_eval(
    json_dataset_path: str,
    template_mcq: str,
    template_mcq_cot_choice: str,
    template_mcq_critique: str,
    template_mcq_critique_completion: str,
    template_mcq_make_choice: str,
    n: int | None,
    system_prompt_behavior: Literal["system", "context"] | None = "system",
    use_cot: bool = False,
    use_self_critique: bool = False,
    self_critique_model: str | None = "openai/gpt-4o-mini",
) -> Task:
    """
    A task which carries out an evaluation.

    Args:
        json_dataset_path : The path where your dataset is saved
        n : the number of samples to use for the evaluation (if None then we use all of them)
        system_prompt_behavior : The behavior of the system prompt, either "system" or "context"
        use_cot : Whether to use chain of thought reasoning
        self_critique_model : The model to use for self-critique

    Returns:
        Task : A task which carries out your evaluation
    """
    dataset = json_dataset(
        json_file=json_dataset_path,
        sample_fields=partial(record_to_sample_full, system_prompt_behavior=system_prompt_behavior),
        limit=n,
    )

    # If not using chain of throught, then just get the answer directly
    if not use_cot:
        assert not use_self_critique, "You can only use self-critique if CoT is enabled."
        solver = chain(
            multiple_choice_format(template=template_mcq),
            generate(),
        )
    else:
        # If using chain of thought, start by generating CoT and an answer
        solver = [multiple_choice_format(template=template_mcq_cot_choice), generate()]

        # If also using self-critique, add this to the solver along with a generation step
        if use_self_critique:
            assert self_critique_model is not None, "You must specify a self-critique model if using self-critique."
            solver = chain(
                solver,
                self_critique_format(
                    model_id=self_critique_model,
                    critique_template=template_mcq_critique,
                    critique_completion_template=template_mcq_critique_completion,
                ),
                generate(),
            )

        # Finally, ask for the answer
        solver = chain(solver, make_choice(prompt=template_mcq_make_choice), generate())

    return Task(
        dataset=dataset,
        solver=solver,
        scorer=answer("letter"),
    )