# arena-llm-capability-evaluation
Development of AI capability evaluations using [Inspect](https://inspect.aisi.org.uk/) from AISI

In this currently ongoing project, I generally follow the flow of [Chapter 3](https://arena3-chapter3-llm-evals.streamlit.app/) in the [ARENA](https://www.arena.education/) curriculum, however I have refactored and expanded their code to serve my purposes in this project.

## Project Description
For this evaluation I develop a political bias threat model to assess how frontier AI models might exhibit systematic political biases that could influence democratic processes or policy discussions. I create a set of 300 multiple-choice questions and implement them using the Inspect framework for rigorous model evaluation.

As a secondary component, I utilize an older (Claude-3.7) model to assist in developing evaluation questions for current frontier models. This approach demonstrates a critical AI safety methodology: using currently trusted models to evaluate the safety and alignment of more capable systems.

## Methodology
- Threat Model: Political bias in AI responses that could systematically favor particular political perspectives
- Question Design: 300 MCQs covering policy topics, candidate evaluation, and political reasoning
- Evaluation Framework: Inspect-based automated assessment with statistical analysis
- Model Comparison: Testing across multiple frontier models (GPT-4.1, Claude-4, etc.)

## How to run the code

## Current Results and Ongoing Work

