# arena-llm-capability-evaluation
Development of AI capability evaluations based on the ARENA Chapter 3 curriculum

In this project I generally follow the flow of Chapter 3 in the ARENA curriculum, however I have refactored and expanded their code to serve my purposes in this project.

## Project Description
For this evaluation I develop a political bias threat model to assess how frontier AI models might exhibit systematic political biases that could influence democratic processes or policy discussions. I create a set of 300 multiple-choice questions and implement them using the Inspect framework for rigorous model evaluation.

As a secondary component, I utilize an older (GPT-3.5/Claude-2 level) model to assist in developing evaluation questions for current frontier models. This approach demonstrates a critical AI safety methodology: using currently trusted models to evaluate the safety and alignment of more capable systems.

## Methodology
- Threat Model: Political bias in AI responses that could systematically favor particular political perspectives
- Question Design: 300 MCQs covering policy topics, candidate evaluation, and political reasoning
- Evaluation Framework: Inspect-based automated assessment with statistical analysis
- Model Comparison: Testing across multiple frontier models (GPT-4, Claude-4, etc.)

## How to run the code

## Current Results and Ongoing Work

